{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb55b425",
   "metadata": {},
   "source": [
    "# Importing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1d365e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import re, mne, numpy as np, pandas as pd\n",
    "import torch, argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "150d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your path to the data\n",
    "main_path = \"../BCI_Database_Processed/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8a882e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def df_attributes():\n",
    "    \n",
    "    #Retrieving attributes of all participants so we can separate them based on specific conditions\n",
    "    attributes_path = glob(\"../BCI_Database/*\")[3]\n",
    "    print(\"Path :\", attributes_path)\n",
    "\n",
    "    attributes = pd.read_excel(attributes_path, header = None)\n",
    "\n",
    "    attributes.columns = attributes.iloc[2]\n",
    "    attributes = attributes.iloc[3:63].reset_index(drop=True)\n",
    "\n",
    "    names = attributes.columns.tolist()\n",
    "    print(\"Columns :\", names)\n",
    "\n",
    "    #Using a dictionnary to fill NA values for every column that contains Na\n",
    "    D = {None : \"Nothing/Unknown\"}\n",
    "    attributes = attributes.replace(D)   \n",
    "    \n",
    "    #Reject every participant that had some issues in the EEG data acquisition\n",
    "    rejected = attributes[attributes['COMMENTS'] != \"Nothing/Unknown\"]['SUJ_ID']\n",
    "    rejected = rejected.tolist()\n",
    "    \n",
    "    return rejected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c52f1702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path : ../BCI_Database\\Perfomances.xlsx\n",
      "Columns : ['SUJ_ID', 'SUJ_gender', 'EXP_gender', 'COMMENTS', 'Perf_RUN_3', 'Perf_RUN_4', 'Perf_RUN_5', 'Perf_RUN_6', 'Birth_year', 'Vision', 'Vision_assistance', 'Symptoms', 'Level of study', 'Level_knowledge neuro', 'Meditation practice', 'Laterality answered', 'Manual activity', 'Manual activity_TXT', 'score', 'time_1', 'time_2', 'PRE_Mood', 'PRE_Mindfulness', 'PRE_Motivation', 'PRE_Hours_sleep_last_night', 'PRE_Usual_sleep', 'PRE_Level_of_alertness', 'PRE_Stimulant_doses_12h', 'PRE_Stimulant_doses_2h', 'PRE_Stim_normal', 'PRE_Tabacco', 'PRE_Tabacco_normal', 'PRE_Alcohol', 'PRE_Last_meal', 'PRE_Last_pills', 'PRE_Pills_TXT', 'POST_Mood', 'POST_Mindfulness', 'POST_Motivation', 'POST_Cognitive load', 'POST_Agentivity', 'POST_Expectations_filled', 'active', 'reflexive', 'sensory', 'intuitive', 'visual', 'verbal', 'sequential', 'global', 'A', 'B', 'C', 'E', 'F', 'G', 'H', 'I', 'L', 'M', 'N', 'O', 'Q1', 'Q2', 'Q3', 'Q4', 'IM', 'EX', 'AX', 'TM', 'IN', 'SC', 'Interrogation']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gricih01\\AppData\\Local\\Temp\\ipykernel_3220\\2529588964.py:17: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  attributes = attributes.replace(D)\n"
     ]
    }
   ],
   "source": [
    "rejected = df_attributes()\n",
    "rejected.append(\"A40\")\n",
    "rejected.append(\"A59\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "afd7515a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def files(main_path, baseline = False, acquisition = False, MI = True, rejected = []):\n",
    "    \n",
    "    #Copying the list since we will remove some subjects in the list\n",
    "    reject = rejected.copy()\n",
    "\n",
    "    single_digit = []\n",
    "    for suj in rejected:\n",
    "        \n",
    "        if len(suj) == 2:\n",
    "            single_digit.append(suj)\n",
    "            reject.remove(suj)\n",
    "        \n",
    "        #Win some time cause 2 digits subjects are after single digit ones\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    #Until -1 to avoid the input_parameters folder\n",
    "    all_file_path = glob(main_path)[:-1]\n",
    "\n",
    "    print(f\"Number of participants : {len(all_file_path)}\\n\")\n",
    "\n",
    "    all_file_path = [path for path in all_file_path if not any(substring in path.split('BCI_Database_Processed')[1] for substring in reject)]\n",
    "    \n",
    "    #Remove A1 because of the acquisition which is shorter than other acquisition files\n",
    "    all_file_path.remove(\"../BCI_Database_Processed\\\\A1\")\n",
    "\n",
    "    #Removing the subject with single digit\n",
    "    for path in single_digit:\n",
    "        all_file_path.remove(f\"../BCI_Database_Processed\\\\{path}\")\n",
    "\n",
    "    print(\"Number of participants retained :\" , len(all_file_path))\n",
    "\n",
    "    all_baseline_path = []\n",
    "    if baseline :\n",
    "        all_baseline_path = []\n",
    "        for path in all_file_path:\n",
    "            path = glob(path + '/*baseline*')\n",
    "            all_baseline_path.append(path)\n",
    "    \n",
    "    all_acquisition_path = []\n",
    "    if acquisition :\n",
    "        for path in all_file_path:\n",
    "            path = glob(path + '/*acquisition*')\n",
    "            all_acquisition_path.append(path)\n",
    "\n",
    "    all_MI_path = []\n",
    "    if MI :\n",
    "        for path in all_file_path:\n",
    "            path = glob(path + '/*online*')\n",
    "            all_MI_path.append(path)\n",
    "\n",
    "    if baseline:\n",
    "        n = len(all_baseline_path)\n",
    "        \n",
    "    elif acquisition:\n",
    "        n = len(all_acquisition_path)\n",
    "\n",
    "    elif MI:\n",
    "        n = len(all_MI_path)\n",
    "\n",
    "    #Creating final list with empty sublists\n",
    "    L = [[] for _ in range(n)]\n",
    "\n",
    "    for i in range(n):\n",
    "\n",
    "        if baseline:\n",
    "            L[i].extend(all_baseline_path[i])\n",
    "\n",
    "        if acquisition:\n",
    "            L[i].extend(all_acquisition_path[i])\n",
    "\n",
    "        if MI:\n",
    "            L[i].extend(all_MI_path[i])\n",
    "    \n",
    "    #Find participants selected\n",
    "    pattern = r'A\\d+'\n",
    "\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8990b0f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants : 60\n",
      "\n",
      "Number of participants retained : 39\n",
      "\n",
      "Number of files per participant : 6\n",
      "Files for participant A10 : ['../BCI_Database_Processed\\\\A10\\\\A10_R1_acquisition.set', '../BCI_Database_Processed\\\\A10\\\\A10_R2_acquisition.set', '../BCI_Database_Processed\\\\A10\\\\A10_R3_onlineT.set', '../BCI_Database_Processed\\\\A10\\\\A10_R4_onlineT.set', '../BCI_Database_Processed\\\\A10\\\\A10_R5_onlineT.set', '../BCI_Database_Processed\\\\A10\\\\A10_R6_onlineT.set']\n"
     ]
    }
   ],
   "source": [
    "all_files = files(main_path, baseline=False, acquisition=True, MI=True, rejected=rejected)\n",
    "print(f\"\\nNumber of files per participant : {len(all_files[0])}\\nFiles for participant A10 : {all_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "abfb3f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of participants : 60\n",
      "\n",
      "Number of participants retained : 39\n",
      "\n",
      "Number of files per participant : 2 \n",
      "Files for participant A10 : ['../BCI_Database_Processed\\\\A10\\\\A10_CE_baseline.set', '../BCI_Database_Processed\\\\A10\\\\A10_OE_baseline.set']\n"
     ]
    }
   ],
   "source": [
    "baseline_files = files(main_path, baseline=True, acquisition=False, MI=False, rejected=rejected)\n",
    "print(f\"\\nNumber of files per participant : {len(baseline_files[0])} \\nFiles for participant A10 : {baseline_files[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d363dd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading data\n",
    "def read_data(file_path):\n",
    "\n",
    "    #Data are already preprocessed so we just load them\n",
    "\n",
    "    # Importing data\n",
    "    data = mne.io.read_raw_eeglab(file_path, eog='auto', preload=True)\n",
    "\n",
    "    #Creating events based on the article annotation (768 = begin of the trial, 769 = left-hand MI, 770 = right-hand MI)\n",
    "    events = mne.events_from_annotations(data, event_id={'Start of Trial, Trigger at t=0s' : 0, 'class1, Left hand\\t- cue onset (BCI experiment)' : 1, 'class2, Right hand\\t- cue onset (BCI experiment)': 2})[0]\n",
    "    \n",
    "    #Creating epochs for every trial\n",
    "    epochs = mne.Epochs(data, events, event_id={'start': 0}, tmin=0, tmax=8, baseline=(0, 2), preload=True).drop_channels(['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'])\n",
    "    \n",
    "    array = epochs['start'].get_data(copy=True)\n",
    "    \n",
    "    #Retrieving the sampling rate\n",
    "    global sfreq\n",
    "    sfreq = int(data.info['sfreq'])\n",
    "\n",
    "    #Retrieving channel names\n",
    "    global channel_names\n",
    "    channel_names = epochs.ch_names\n",
    "\n",
    "    print(f\"\\nSampling rate : {sfreq}\")\n",
    "    return array, events\n",
    "\n",
    "def read_baseline(file_path):\n",
    "\n",
    "    # Importing data\n",
    "    data = mne.io.read_raw_eeglab(file_path, eog='auto', preload=True)\n",
    "\n",
    "    #Creating epochs (same # of epochs as the old one)\n",
    "    epochs = mne.make_fixed_length_epochs(data, duration = 4.6, overlap = 0, preload=True).drop_channels(['EOG1', 'EOG2', 'EOG3', 'EMGg', 'EMGd'])\n",
    "\n",
    "    array = epochs.get_data()\n",
    "\n",
    "    return array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3927aefe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_all_data(all_files):\n",
    "\n",
    "    #Label array to store every ID for each participant\n",
    "    label_subject_acquisition = []\n",
    "    label_subject_MI = []\n",
    "\n",
    "    #Regex pattern to find the ID of the subject\n",
    "    pattern = r'A\\d+'\n",
    "\n",
    "    data_acquisition_list = []  #Metadata of the acquisition\n",
    "    data_MI_list = []           #Metadata of the MI\n",
    "\n",
    "    events_acquisition = []            #Storing the events for every signal\n",
    "    events_MI = []            #Storing the events for every signal\n",
    "\n",
    "    for subject_files in all_files:\n",
    "        \n",
    "        #Storing acquisition for each subject (each signal selected)\n",
    "        data_subject_acquisition = []\n",
    "\n",
    "        #Storing MI for each subject\n",
    "        data_subject_MI = []\n",
    "        label = re.findall(pattern, subject_files[0])[0]\n",
    "\n",
    "        for j in subject_files:\n",
    "\n",
    "            data, event = read_data(j)\n",
    "\n",
    "            if 'acquisition' in j:\n",
    "                print(j)\n",
    "                data_subject_acquisition.append(data)\n",
    "                events_acquisition.append(event)\n",
    "\n",
    "            elif 'onlineT' in j:\n",
    "                data_subject_MI.append(data)\n",
    "                events_MI.append(event)\n",
    "\n",
    "        \n",
    "        #Creating the array and reshaping it (#files, # epochs, #channels, resolution) -> (#epochs * #files, #channels, #resolution)\n",
    "        data_subject_acquisition = np.array(data_subject_acquisition)\n",
    "\n",
    "        #Reshaping acquisition matrix\n",
    "        files_acquisition, epochs_acquisition, channels_acquisition, resolution_acquisition = data_subject_acquisition.shape\n",
    "        data_subject_acquisition = data_subject_acquisition.reshape((files_acquisition*epochs_acquisition, channels_acquisition, resolution_acquisition))\n",
    "        \n",
    "        #Adding the labels to the arrays\n",
    "        label_subject_acquisition.append([label] *  (files_acquisition*epochs_acquisition))\n",
    "\n",
    "        #Adding it to the metadata\n",
    "        data_acquisition_list.append(data_subject_acquisition)\n",
    "\n",
    "\n",
    "        #Same for motor imagery\n",
    "        data_subject_MI = np.array(data_subject_MI)\n",
    "\n",
    "        #Reshaping motor imagery matrix\n",
    "        files_MI, epochs_MI, channels_MI, resolution_MI = data_subject_MI.shape\n",
    "        data_subject_MI = data_subject_MI.reshape((files_MI*epochs_MI, channels_MI, resolution_MI))\n",
    "\n",
    "        #Adding the labels to the arrays\n",
    "        label_subject_MI.append([label] *  (files_MI*epochs_MI))\n",
    "\n",
    "        #Adding it to the metadata\n",
    "        data_MI_list.append(data_subject_MI)\n",
    "        \n",
    "    #Creating the array and reshaping it to the good shape\n",
    "    events_acquisition = np.array(events_acquisition)\n",
    "    events_acquisition = events_acquisition.reshape(-1, 3)\n",
    "\n",
    "    events_MI = np.array(events_MI)\n",
    "    events_MI = events_MI.reshape(-1, 3)\n",
    "\n",
    "    return np.array(data_acquisition_list), np.array(data_MI_list), np.array(label_subject_acquisition), np.array(label_subject_MI), events_acquisition, events_MI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cac41520",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "#Execution takes around 6mins\n",
    "data_array_acquisition, data_array_MI, label_array_acquisition, label_array_MI, events_acquisition, events_MI = read_all_data(all_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ed253a59",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 80, 27, 4097)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_array_acquisition.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1f259ec",
   "metadata": {},
   "source": [
    "# Anonymizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "52081f2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gricih01\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts\\MI_BCI_Anonymizing_data\n",
      "['C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\python311.zip', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\DLLs', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0', '', 'C:\\\\Users\\\\gricih01\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages', 'C:\\\\Users\\\\gricih01\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32', 'C:\\\\Users\\\\gricih01\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\win32\\\\lib', 'C:\\\\Users\\\\gricih01\\\\AppData\\\\Local\\\\Packages\\\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\\\LocalCache\\\\local-packages\\\\Python311\\\\site-packages\\\\Pythonwin', 'C:\\\\Program Files\\\\WindowsApps\\\\PythonSoftwareFoundation.Python.3.11_3.11.2544.0_x64__qbz5n2kfra8p0\\\\Lib\\\\site-packages', 'C:\\\\Users\\\\gricih01\\\\OneDrive - Université du Québec en Outaouais\\\\Documents\\\\MI BCI Database and scripts\\\\Scripts\\\\unlearnable_privacy-master'] \n",
      "\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Obtenez le chemin du répertoire Scripts\n",
    "scripts_dir = r'C:\\Users\\gricih01\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts\\MI_BCI_Anonymizing_data'\n",
    "print(scripts_dir)\n",
    "\n",
    "# Obtenez le chemin du répertoire unlearnable-privacy-master\n",
    "parent_dir = r'C:\\Users\\gricih01\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts'\n",
    "unlearnable_privacy_dir = r'C:\\Users\\gricih01\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts\\unlearnable_privacy-master'\n",
    "\n",
    "# Ajoutez le répertoire unlearnable-privacy-master au chemin système\n",
    "sys.path.append(unlearnable_privacy_dir)\n",
    "\n",
    "# Vérifiez si le chemin est correct\n",
    "print(sys.path, '\\n\\nDone')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2fc161d",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = data_array_acquisition\n",
    "\n",
    "y_train = events_acquisition[(events_acquisition[:, 2] == 1) | (events_acquisition[:, 2] == 2), -1]\n",
    "y_train = y_train.reshape(39, 80)\n",
    "\n",
    "s_train = label_array_acquisition "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7d5eecc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser(description='Model train')\n",
    "parser.add_argument('--gpu_id', type=str, default='1')\n",
    "parser.add_argument('--dataset', type=str, default='EPFLnoClip')\n",
    "\n",
    "parser.add_argument('--batch_size', type=int, default=128)\n",
    "parser.add_argument('--epochs', type=int, default=150)\n",
    "parser.add_argument('--optim', type=str, default='Adam')\n",
    "parser.add_argument('--lr', type=float, default=0.01)\n",
    "\n",
    "#We comment one of these two lines since they do not work (Jupyter conflict)\n",
    "#parser.add_argument('--feature_c', type=str, default='EEGNet')\n",
    "parser.add_argument('--feature_d', type=str, default='EEGNet')\n",
    "\n",
    "parser.add_argument('--subject_wise', type=bool, default=False)\n",
    "parser.add_argument('--alpha', type=float, default=1.0) \n",
    "parser.add_argument('--log', type=str, default='')\n",
    "\n",
    "args = parser.parse_args()\n",
    "args.device = f'cuda:{args.gpu_id}' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b34cff47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gpu_id: 1\n",
      "dataset: EPFLnoClip\n",
      "batch_size: 128\n",
      "epochs: 150\n",
      "optim: Adam\n",
      "lr: 0.01\n",
      "feature_d: EEGNet\n",
      "subject_wise: False\n",
      "alpha: 1.0\n",
      "log: \n",
      "device: cpu\n",
      "feature_c: EEGNet\n"
     ]
    }
   ],
   "source": [
    "args.feature_c = 'EEGNet'\n",
    "args.feature_d = 'EEGNet'\n",
    "\n",
    "for arg in vars(args):\n",
    "    print(f\"{arg}: {getattr(args, arg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9bdbd9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def str_to_int(S):\n",
    "    #To convert string labels into int since\n",
    "    x, y = S.shape\n",
    "    S_int = np.zeros((x, y))\n",
    "\n",
    "    for i in range(x):\n",
    "        for j in range(y):\n",
    "            \n",
    "            #Recuperating the integer of the subject\n",
    "            subject = S[i, j][1:]\n",
    "\n",
    "            S_int[i, j] = subject\n",
    "\n",
    "    return S_int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b893dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10. 10.\n",
      " 10. 10. 10. 10. 10. 10. 10. 10.] (39, 80)\n"
     ]
    }
   ],
   "source": [
    "s_train = str_to_int(s_train)\n",
    "print(s_train[0, :], s_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c1a903ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(39, 80, 27, 4097) (39, 80) (39, 80)\n"
     ]
    }
   ],
   "source": [
    "#s_train = np.hstack(s_train)\n",
    "\n",
    "#x_train have to be 4-dimensionals (otherwise the code below will not work)\n",
    "print(x_train.shape, s_train.shape, y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f1bde8a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of x_train : torch.Size([39, 80, 27, 4097])\n",
      "Type of x_merged : <class 'torch.Tensor'>\n",
      "\n",
      "shape of y_sorted : torch.Size([39, 80])\n",
      "Type of y_sorted : <class 'torch.Tensor'>\n",
      "\n",
      "shape of s_train : torch.Size([39, 80])\n",
      "Type of s_train : <class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "x_train = torch.FloatTensor(x_train)\n",
    "x_merged = x_train.view(39 * 80, 27, 4097)\n",
    "print(f\"shape of x_train : {x_train.shape}\")\n",
    "print(f\"Type of x_merged : {type(x_train)}\\n\")\n",
    "\n",
    "\n",
    "y_train = torch.IntTensor(y_train)\n",
    "y_sorted = y_train -1\n",
    "print(f\"shape of y_sorted : {y_sorted.shape}\")\n",
    "print(f\"Type of y_sorted : {type(y_sorted)}\\n\")\n",
    "\n",
    "\n",
    "s_train = torch.IntTensor(s_train)\n",
    "print(f\"shape of s_train : {s_train.shape}\")\n",
    "print(f\"Type of s_train : {type(s_train)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ec682de1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'utils.pytorch_utils' from 'C:\\\\Users\\\\gricih01\\\\OneDrive - Université du Québec en Outaouais\\\\Documents\\\\MI BCI Database and scripts\\\\Scripts\\\\unlearnable_privacy-master\\\\utils\\\\pytorch_utils.py'>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Since we're modifying their code, we need to reimport the library for our tests\n",
    "import importlib\n",
    "import unlearnable_gen\n",
    "import utils.pytorch_utils\n",
    "from models import LoadModel, Classifier, Discriminator, CalculateOutSize\n",
    "\n",
    "importlib.reload(unlearnable_gen)\n",
    "importlib.reload(utils.pytorch_utils)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "73358726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of x_merged : torch.Size([3120, 27, 4097])\n",
      "39 39\n",
      "39 39\n",
      "39 39\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Classifier(\n",
       "  (block): Sequential(\n",
       "    (0): Linear(in_features=776, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(f\"Shape of x_merged : {x_merged.shape}\")\n",
    "\n",
    "tensors = [x_train, y_sorted, s_train]\n",
    "for tensor in tensors:\n",
    "    print(tensors[0].size(0), tensor.size(0))\n",
    "\n",
    "\n",
    "\n",
    "assert all(tensors[0].size(0) == tensor.size(0) for tensor in tensors), \"Problème de tailles\"\n",
    "\n",
    "# Assurez-vous que x_train est de la forme (3120, 1, 27, 4097)\n",
    "x_merged = x_merged.reshape(-1, 1, 27, 4097)\n",
    "\n",
    "\n",
    "chans, samples = x_merged.shape[1], x_merged.shape[0]\n",
    "\n",
    "feature_ext = LoadModel(model_name=args.feature_d, Chans=chans, Samples=samples)\n",
    "\n",
    "#feature_ext\n",
    "\n",
    "classifier = Classifier(\n",
    "    input_dim=CalculateOutSize(feature_ext, chans, samples),\n",
    "    n_classes=len(np.unique(y_sorted.numpy()))).to(args.device)\n",
    "discriminator = Discriminator(\n",
    "    input_dim=CalculateOutSize(feature_ext, chans, samples),\n",
    "    n_subjects=len(np.unique(s_train.numpy()))).to(args.device)\n",
    "\n",
    "classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "54e265e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([39, 80])\n",
      "tensor([[0, 1, 0,  ..., 1, 0, 0],\n",
      "        [1, 0, 1,  ..., 0, 1, 0],\n",
      "        [0, 0, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 1, 0, 0],\n",
      "        [1, 1, 1,  ..., 1, 1, 0],\n",
      "        [1, 0, 0,  ..., 1, 1, 0]], dtype=torch.int32)\n",
      "tensor([[1, 2, 1,  ..., 2, 1, 1],\n",
      "        [2, 1, 2,  ..., 1, 2, 1],\n",
      "        [1, 1, 2,  ..., 1, 1, 1],\n",
      "        ...,\n",
      "        [2, 2, 2,  ..., 2, 1, 1],\n",
      "        [2, 2, 2,  ..., 2, 2, 1],\n",
      "        [2, 1, 1,  ..., 2, 2, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(y_sorted.shape)\n",
    "print(y_sorted)\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30579e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "for ten\n",
    "    count = [0.0] * len(np.unique(y.numpy()))\n",
    "    for label in y:\n",
    "        count[label] += 1.0\n",
    "    count = [len(y) / x for x in count]\n",
    "    weight = [0.0] * len(y)\n",
    "    for idx, label in enumerate(y):\n",
    "        weight[idx] = count[label]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df600b85",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer tensors of a single element can be converted to an index",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Import function from .py files\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# x_train: raw EEG training data, y_sorted: task labels, s_train: identity labels\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m \n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# generated by subject-wise noise\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m u_x_train \u001b[38;5;241m=\u001b[39m \u001b[43munlearnable_gen\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munlearnable_optim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_sorted\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts\\unlearnable_privacy-master\\unlearnable_gen.py:245\u001b[0m, in \u001b[0;36munlearnable_optim\u001b[1;34m(x, y_label, s_label, args)\u001b[0m\n\u001b[0;32m    242\u001b[0m classifier\u001b[38;5;241m.\u001b[39mapply(init_weights)\n\u001b[0;32m    243\u001b[0m discriminator\u001b[38;5;241m.\u001b[39mapply(init_weights)\n\u001b[1;32m--> 245\u001b[0m sample_weights \u001b[38;5;241m=\u001b[39m \u001b[43mweight_for_balanced_classes\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_label\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    246\u001b[0m sampler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39msampler\u001b[38;5;241m.\u001b[39mWeightedRandomSampler(\n\u001b[0;32m    247\u001b[0m     sample_weights, \u001b[38;5;28mlen\u001b[39m(sample_weights))\n\u001b[0;32m    248\u001b[0m data_loader \u001b[38;5;241m=\u001b[39m DataLoader(dataset\u001b[38;5;241m=\u001b[39mTensorDataset(x, y_label, s_label),\n\u001b[0;32m    249\u001b[0m                          batch_size\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mbatch_size,\n\u001b[0;32m    250\u001b[0m                          sampler\u001b[38;5;241m=\u001b[39msampler,\n\u001b[0;32m    251\u001b[0m                          drop_last\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\OneDrive - Université du Québec en Outaouais\\Documents\\MI BCI Database and scripts\\Scripts\\unlearnable_privacy-master\\utils\\pytorch_utils.py:116\u001b[0m, in \u001b[0;36mweight_for_balanced_classes\u001b[1;34m(y)\u001b[0m\n\u001b[0;32m    114\u001b[0m count \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(y\u001b[38;5;241m.\u001b[39mnumpy()))\n\u001b[0;32m    115\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m y:\n\u001b[1;32m--> 116\u001b[0m     \u001b[43mcount\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n\u001b[0;32m    117\u001b[0m count \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mlen\u001b[39m(y) \u001b[38;5;241m/\u001b[39m x \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m count]\n\u001b[0;32m    118\u001b[0m weight \u001b[38;5;241m=\u001b[39m [\u001b[38;5;241m0.0\u001b[39m] \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(y)\n",
      "\u001b[1;31mTypeError\u001b[0m: only integer tensors of a single element can be converted to an index"
     ]
    }
   ],
   "source": [
    "# Import function from .py files\n",
    "\n",
    "# x_train: raw EEG training data, y_sorted: task labels, s_train: identity labels\n",
    "# generated by sample-wise noise\n",
    "#u_x_train = unlearnable_gen.unlearnable(x_train, y_train, s_train, args)\n",
    "\n",
    "# generated by subject-wise noise\n",
    "u_x_train = unlearnable_gen.unlearnable_optim(x_train, y_sorted, s_train, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a152598",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mFailed to start the Kernel. \n",
      "\u001b[1;31mUnable to start Kernel 'Python 3.11.9' due to a timeout waiting for the ports to get used. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
